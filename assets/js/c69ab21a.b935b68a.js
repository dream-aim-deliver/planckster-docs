"use strict";(self.webpackChunkplanckster_docs=self.webpackChunkplanckster_docs||[]).push([[7676],{6528:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>c,toc:()=>o});var r=i(4848),s=i(8453);const l={sidebar_label:"Data Scrapers",sidebar_position:3},t="Telegram Scraper",c={id:"concepts/scrapers",title:"Telegram Scraper",description:"Arguments & Inputs",source:"@site/docs/concepts/scrapers.md",sourceDirName:"concepts",slug:"/concepts/scrapers",permalink:"/planckster-docs/docs/concepts/scrapers",draft:!1,unlisted:!1,editUrl:"https://github.com/dream-aim-deliver/planckster-docs/edit/main/docs/concepts/scrapers.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_label:"Data Scrapers",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Architecture Overview",permalink:"/planckster-docs/docs/concepts/architecture"},next:{title:"Disaster Tracking Usecase",permalink:"/planckster-docs/docs/concepts/disaster-tracking-usecase"}},d={},o=[{value:"Arguments &amp; Inputs",id:"arguments--inputs",level:3},{value:"Kernel-Planckster-specific arguments:",id:"kernel-planckster-specific-arguments",level:4},{value:"Scraper-specific arguments:",id:"scraper-specific-arguments",level:4},{value:"Telegram API Configuration:",id:"telegram-api-configuration",level:5},{value:"Telegram &amp; Scraping",id:"telegram--scraping",level:3},{value:"Configuration",id:"configuration",level:4},{value:"Retrieving Messages",id:"retrieving-messages",level:4},{value:"Augmentation",id:"augmentation",level:2},{value:"Utility Functions",id:"utility-functions",level:2},{value:"How to Run Locally",id:"how-to-run-locally",level:2},{value:"Example",id:"example",level:2},{value:"Arguments &amp; Inputs",id:"arguments--inputs-1",level:3},{value:"Kernel-Planckster-specific arguments:",id:"kernel-planckster-specific-arguments-1",level:4},{value:"Scraper-specific arguments:",id:"scraper-specific-arguments-1",level:4},{value:"Twitter API Configuration:",id:"twitter-api-configuration",level:5},{value:"Twitter &amp; Scraping",id:"twitter--scraping",level:3},{value:"Configuration",id:"configuration-1",level:4},{value:"Retrieving Tweets",id:"retrieving-tweets",level:4},{value:"Augmentation",id:"augmentation-1",level:2},{value:"Utility Functions",id:"utility-functions-1",level:2},{value:"How to Run Locally",id:"how-to-run-locally-1",level:2},{value:"Example",id:"example-1",level:2},{value:"Scraper",id:"scraper",level:2},{value:"Arguments &amp; Inputs",id:"arguments--inputs-2",level:3},{value:"Kernel-Planckster-specific arguments:",id:"kernel-planckster-specific-arguments-2",level:4},{value:"Scraper-specific arguments:",id:"scraper-specific-arguments-2",level:4},{value:"Define the WGS84 coordinate bounding box; float",id:"define-the-wgs84-coordinate-bounding-box-float",level:5},{value:"Selecting the date range: &quot;YYYY-MM-DD&quot; format",id:"selecting-the-date-range-yyyy-mm-dd-format",level:5},{value:"Spatial Resolution: integer value (meters)",id:"spatial-resolution-integer-value-meters",level:5},{value:"Sentinelhub &amp; Scraping",id:"sentinelhub--scraping",level:3},{value:"Configuration",id:"configuration-2",level:4},{value:"Retrieving Images",id:"retrieving-images",level:4},{value:"Augmentation",id:"augmentation-2",level:2},{value:"Utility Functions",id:"utility-functions-2",level:2},{value:"How to Run Locally",id:"how-to-run-locally-2",level:2},{value:"Example",id:"example-2",level:2}];function a(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"telegram-scraper",children:"Telegram Scraper"}),"\n",(0,r.jsx)(n.h3,{id:"arguments--inputs",children:"Arguments & Inputs"}),"\n",(0,r.jsx)(n.h4,{id:"kernel-planckster-specific-arguments",children:"Kernel-Planckster-specific arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tracer_id"}),": Identifier for tracing the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_host"}),": Kernel Planckster host."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_port"}),": Kernel Planckster port."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_auth_token"}),": Kernel Planckster authentication token."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_scheme"}),": Kernel Planckster scheme (http/https)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"log_level"}),": Logging level for the scraper."]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"scraper-specific-arguments",children:"Scraper-specific arguments:"}),"\n",(0,r.jsx)(n.h5,{id:"telegram-api-configuration",children:"Telegram API Configuration:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_api_id"}),": Telegram API ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_api_hash"}),": Telegram API Hash."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_phone_number"}),": Phone number linked to the Telegram account (optional)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_password"}),": Password for the Telegram account (optional)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_bot_token"}),": Telegram bot token (optional)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"channel_name"}),": Name of the Telegram channel to scrape."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"telegram--scraping",children:"Telegram & Scraping"}),"\n",(0,r.jsx)(n.h4,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"get_scraping_client(job_id, logger, telegram_api_id, telegram_api_hash, telegram_phone_number=None, telegram_password=None, telegram_bot_token=None) -> TelegramClient"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This function sets up the Telegram client (using Telethon python package)."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"logger"}),": Logger instance."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_api_id"}),": Telegram API ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_api_hash"}),": Telegram API Hash."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_phone_number"}),": Phone number linked to the Telegram account (optional)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_password"}),": Password for the Telegram account (optional)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_bot_token"}),": Telegram bot token (optional)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A configured ",(0,r.jsx)(n.code,{children:"TelegramClient"})," instance."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"retrieving-messages",children:"Retrieving Messages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scrape(job_id, channel_name, tracer_id, scraped_data_repository, telegram_client, openai_api_key, log_level) -> JobOutput"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This function takes as input the arguments required to scrape the desired data using the Telegram Scraper API."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"channel_name"}),": Name of the Telegram channel."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tracer_id"}),": Identifier for tracing the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scraped_data_repository"}),": Repository to store scraped data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"telegram_client"}),": Configured ",(0,r.jsx)(n.code,{children:"TelegramClient"})," instance."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"openai_api_key"}),": API key for OpenAI services."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"log_level"}),": Logging level."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A ",(0,r.jsx)(n.code,{children:"JobOutput"})," object with the job state and list of source data."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"augmentation",children:"Augmentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"augment_telegram(client, message, filter)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Once the text messages are extracted, this function will handle the augmentation of this data with other data sources for example data scraped through Sentinel API."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"client"}),": Configured Instructor client."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"message"}),": Telegram message to be processed."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"filter"}),": Filter string for message relevance."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A list of augmented data if relevant, otherwise ",(0,r.jsx)(n.code,{children:"None"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"get_lat_long(location_name)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Retrieves latitude and longitude for a given location name using geolocation."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"location_name"}),": Name of the location to retrieve coordinates for."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Latitude and longitude coordinates."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"utility-functions",children:"Utility Functions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Extensive Logging and Error Handling"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The scraper includes in-built error handling mechanisms (",(0,r.jsx)(n.code,{children:"try-except"})," blocks) and logs various stages of the job execution."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Job State Management"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The job state is tracked and updated through various stages (",(0,r.jsx)(n.code,{children:"BaseJobState.CREATED"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.RUNNING"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.FINISHED"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.FAILED"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cleanup"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"After processing, temporary directories where media files are saved are cleaned up to free up storage."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"how-to-run-locally",children:"How to Run Locally"}),"\n",(0,r.jsxs)(n.p,{children:["To run the Telegram scraper locally, follow the instructions in the ",(0,r.jsx)(n.a,{href:"#",children:"Local Setup Guide"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nfrom telethon import TelegramClient\n\n# Set up logging\nlogger = logging.getLogger(\'telegram_scraper\')\nlogging.basicConfig(level=logging.INFO)\n\n# Define arguments\njob_id = 123\ntracer_id = "abc123"\ntelegram_api_id = "your_api_id"\ntelegram_api_hash = "your_api_hash"\ntelegram_phone_number = "your_phone_number"\ntelegram_password = "your_password"\ntelegram_bot_token = "your_bot_token"\nchannel_name = "your_channel_name"\nopenai_api_key = "your_openai_api_key"\nlog_level = logging.INFO\n\n# Set up the Telegram client\nclient = get_scraping_client(job_id, logger, telegram_api_id, telegram_api_hash, telegram_phone_number, telegram_password, telegram_bot_token)\n\n# Create a ScrapedDataRepository instance (implement as needed)\nscraped_data_repository = ScrapedDataRepository()\n\n# Run the scraper\noutput = await scrape(job_id, channel_name, tracer_id, scraped_data_repository, client, openai_api_key, log_level)\n\n# Output the result\nprint(output)\n'})}),"\n",(0,r.jsx)(n.h1,{id:"twitter-scraper",children:"Twitter Scraper"}),"\n",(0,r.jsx)(n.h3,{id:"arguments--inputs-1",children:"Arguments & Inputs"}),"\n",(0,r.jsx)(n.h4,{id:"kernel-planckster-specific-arguments-1",children:"Kernel-Planckster-specific arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tracer_id"}),": Identifier for tracing the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_host"}),": Kernel Planckster host."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_port"}),": Kernel Planckster port."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_auth_token"}),": Kernel Planckster authentication token."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_scheme"}),": Kernel Planckster scheme (http/https)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"log_level"}),": Logging level for the scraper."]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"scraper-specific-arguments-1",children:"Scraper-specific arguments:"}),"\n",(0,r.jsx)(n.h5,{id:"twitter-api-configuration",children:"Twitter API Configuration:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"query"}),": Search query for Twitter."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"start_date"}),": Start date for the search (YYYY-MM-DD)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"end_date"}),": End date for the search (YYYY-MM-DD)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scraper_api_key"}),": API key for the scraper service."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"openai_api_key"}),": API key for OpenAI services."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"twitter--scraping",children:"Twitter & Scraping"}),"\n",(0,r.jsx)(n.h4,{id:"configuration-1",children:"Configuration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"setup(job_id, logger, kp_auth_token, kp_host, kp_port, kp_scheme) -> Tuple[KernelPlancksterGateway, ProtocolEnum, FileRepository]"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This function sets up the Kernel Planckster Gateway, the storage protocol, and the file repository."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"logger"}),": Logger instance."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_auth_token"}),": Kernel Planckster authentication token."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_host"}),": Kernel Planckster host."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_port"}),": Kernel Planckster port."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_scheme"}),": Kernel Planckster scheme (http/https)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A tuple containing the Kernel Planckster Gateway, storage protocol, and file repository."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"retrieving-tweets",children:"Retrieving Tweets"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scrape(job_id, tracer_id, query, start_date, end_date, scraped_data_repository, work_dir, log_level, scraper_api_key, openai_api_key) -> JobOutput"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This function takes as input the arguments required to scrape the desired data using the Twitter Scraper API."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tracer_id"}),": Identifier for tracing the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"query"}),": Search query for Twitter."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"start_date"}),": Start date for the search (YYYY-MM-DD)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"end_date"}),": End date for the search (YYYY-MM-DD)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scraped_data_repository"}),": Repository to store scraped data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"work_dir"}),": Directory for temporary work files."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"log_level"}),": Logging level."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scraper_api_key"}),": API key for the scraper service."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"openai_api_key"}),": API key for OpenAI services."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A ",(0,r.jsx)(n.code,{children:"JobOutput"})," object with the job state and list of source data."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"augmentation-1",children:"Augmentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"augment_tweet(client, tweet, filter)"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Once the tweets are extracted, this function will handle the augmentation of this data with other data sources for example data scraped through Sentinel API."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Arguments:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"client"}),": Configured Instructor client."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tweet"}),": Tweet data to be processed."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"filter"}),": Filter string for tweet relevance."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A list of augmented data if relevant, otherwise ",(0,r.jsx)(n.code,{children:"None"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"get_lat_long(location_name)"}),":"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Retrieves latitude and longitude for a given location name using geolocation."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Arguments:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"location_name"}),": Name of the location to retrieve coordinates for."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Latitude and longitude coordinates."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"utility-functions-1",children:"Utility Functions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Extensive Logging and Error Handling"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The scraper includes in-built error handling mechanisms (",(0,r.jsx)(n.code,{children:"try-except"})," blocks) and logs various stages of the job execution."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Job State Management"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The job state is tracked and updated through various stages (",(0,r.jsx)(n.code,{children:"BaseJobState.CREATED"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.RUNNING"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.FINISHED"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.FAILED"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cleanup"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"After processing, temporary directories where media files are saved are cleaned up to free up storage."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"save_tweets(tweets, file_path)"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Saves tweets to the specified file path in JSON format."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tweets"}),": List of tweets to be saved."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"file_path"}),": Path to save the tweets."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"load_tweets(file_path)"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Loads tweets from the specified file path."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"file_path"}),": Path to load the tweets from."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Loaded tweet data."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"how-to-run-locally-1",children:"How to Run Locally"}),"\n",(0,r.jsxs)(n.p,{children:["To run the Twitter scraper locally, follow the instructions in the ",(0,r.jsx)(n.a,{href:"#",children:"Local Setup Guide"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"example-1",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nfrom app.sdk.scraped_data_repository import ScrapedDataRepository\n\n# Set up logging\nlogger = logging.getLogger(\'twitter_scraper\')\nlogging.basicConfig(level=logging.INFO)\n\n# Define arguments\njob_id = 123\ntracer_id = "abc123"\nquery = "forest fire"\nstart_date = "2023-01-01"\nend_date = "2023-01-31"\nwork_dir = "/path/to/work/dir"\nlog_level = logging.INFO\nscraper_api_key = "your_scraper_api_key"\nopenai_api_key = "your_openai_api_key"\n\n# Create a ScrapedDataRepository instance (implement as needed)\nscraped_data_repository = ScrapedDataRepository()\n\n# Run the scraper\noutput = scrape(job_id, tracer_id, query, start_date, end_date, scraped_data_repository, work_dir, log_level, scraper_api_key, openai_api_key)\n\n# Output the result\nprint(output)\n\n'})}),"\n",(0,r.jsx)(n.h1,{id:"sentinel-scraper-documentation",children:"Sentinel Scraper Documentation"}),"\n",(0,r.jsx)(n.h2,{id:"scraper",children:"Scraper"}),"\n",(0,r.jsx)(n.h3,{id:"arguments--inputs-2",children:"Arguments & Inputs"}),"\n",(0,r.jsx)(n.h4,{id:"kernel-planckster-specific-arguments-2",children:"Kernel-Planckster-specific arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tracer_id"}),": Identifier for tracing the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_host"}),": Kernel Planckster host."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_port"}),": Kernel Planckster port."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_auth_token"}),": Kernel Planckster authentication token."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"kp_scheme"}),": Kernel Planckster scheme (http/https)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"log_level"}),": Logging level for the scraper."]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"scraper-specific-arguments-2",children:"Scraper-specific arguments:"}),"\n",(0,r.jsx)(n.h5,{id:"define-the-wgs84-coordinate-bounding-box-float",children:"Define the WGS84 coordinate bounding box; float"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"long_left"}),": Leftmost longitude, e.g., -113.2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"lat_down"}),": Bottommost latitude, e.g., 57.2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"long_right"}),": Rightmost longitude, e.g., -108.5"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"lat_up"}),": Topmost latitude, e.g., 59.3"]}),"\n"]}),"\n",(0,r.jsx)(n.h5,{id:"selecting-the-date-range-yyyy-mm-dd-format",children:'Selecting the date range: "YYYY-MM-DD" format'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"start_date"}),': Day to begin scanning, e.g., "2023-06-01"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"end_date"}),': Day to finish scanning, e.g., "2023-06-20"\n',(0,r.jsx)(n.em,{children:"Note:"})," Depending on the specific satellite (e.g., Landsat, Sentinel, etc.), the temporal resolution varies. It\u2019s unlikely to get a new image every day in a region, so using large timeframes and multiple satellites is the best way to receive coverage."]}),"\n"]}),"\n",(0,r.jsx)(n.h5,{id:"spatial-resolution-integer-value-meters",children:"Spatial Resolution: integer value (meters)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"resolution"}),": Meters above the surface, e.g., 120\n",(0,r.jsx)(n.em,{children:"Note:"})," Resolution must be greater than 10 meters and the final image cannot exceed 2500 x 2500 pixels. Thus, one can either have high resolution (fewer meters above the surface) and a small bounding box or low resolution (more meters above the surface) and a large bounding box."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sentinelhub--scraping",children:"Sentinelhub & Scraping"}),"\n",(0,r.jsx)(n.h4,{id:"configuration-2",children:"Configuration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"get_scraping_config(job_id, logger, sentinel_client_id, sentinel_client_secret) -> SHConfig"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This function sets up the Sentinel client configuration (using SentinelHub API)."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"logger"}),": Logger instance."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sentinel_client_id"}),": SentinelHub API Client ID."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sentinel_client_secret"}),": SentinelHub API Client Secret."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A configured ",(0,r.jsx)(n.code,{children:"SHConfig"})," instance."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"retrieving-images",children:"Retrieving Images"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"get_images(logger, job_id, tracer_id, scraped_data_repository, output_data_list, protocol, coords_wgs84, evalscript_true_color, config, start_date, end_date, resolution, image_dir) -> list[KernelPlancksterSourceData]"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This function takes as input the arguments required to scrape the desired data using the Sentinel API."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"logger"}),": Logger instance."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tracer_id"}),": Identifier for tracing the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scraped_data_repository"}),": Repository to store scraped data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"output_data_list"}),": List to store output data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"protocol"}),": Storage protocol."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"coords_wgs84"}),": Tuple of coordinates (long_left, lat_down, long_right, lat_up)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"evalscript_true_color"}),": Evalscript for Sentinel Hub request."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"config"}),": Sentinel Hub configuration object."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"start_date"}),": Start date for image retrieval."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"end_date"}),": End date for image retrieval."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"resolution"}),": Spatial resolution in meters."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"image_dir"}),": Directory to save images."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A list of ",(0,r.jsx)(n.code,{children:"KernelPlancksterSourceData"})," objects."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"augmentation-2",children:"Augmentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"augment_images(logger, job_id, tracer_id, scraped_data_repository, output_data_list, protocol, coords_wgs84, image_dir) -> list[KernelPlancksterSourceData]"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This function processes the images to detect certain features, such as forest fires."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Arguments:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"logger"}),": Logger instance."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"job_id"}),": Unique identifier for the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"tracer_id"}),": Identifier for tracing the job."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"scraped_data_repository"}),": Repository to store scraped data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"output_data_list"}),": List to store output data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"protocol"}),": Storage protocol."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"coords_wgs84"}),": Tuple of coordinates (long_left, lat_down, long_right, lat_up)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"image_dir"}),": Directory where images are saved."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A list of ",(0,r.jsx)(n.code,{children:"KernelPlancksterSourceData"})," objects."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"utility-functions-2",children:"Utility Functions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Extensive Logging and Error Handling"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The scraper includes in-built error handling mechanisms (",(0,r.jsx)(n.code,{children:"try-except"})," blocks) and logs various stages of the job execution."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Job State Management"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The job state is tracked and updated through various stages (",(0,r.jsx)(n.code,{children:"BaseJobState.CREATED"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.RUNNING"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.FINISHED"}),", ",(0,r.jsx)(n.code,{children:"BaseJobState.FAILED"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cleanup"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"After processing, temporary directories where images are saved are cleaned up to free up storage."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"how-to-run-locally-2",children:"How to Run Locally"}),"\n",(0,r.jsxs)(n.p,{children:["To run the Sentinel scraper locally, follow the instructions in the ",(0,r.jsx)(n.a,{href:"#",children:"Local Setup Guide"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"example-2",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import logging\nfrom sentinelhub import SHConfig\nfrom app.sdk.scraped_data_repository import ScrapedDataRepository\n\n# Set up logging\nlogger = logging.getLogger(\'sentinel_scraper\')\nlogging.basicConfig(level=logging.INFO)\n\n# Define arguments\njob_id = 123\ntracer_id = "abc123"\nlong_left = -113.2\nlat_down = 57.2\nlong_right = -108.5\nlat_up = 59.3\nstart_date = "2023-06-01"\nend_date = "2023-06-20"\nresolution = 120\nimage_dir = "/path/to/image_dir"\nlog_level = logging.INFO\nsentinel_client_id = "your_sentinel_client_id"\nsentinel_client_secret = "your_sentinel_client_secret"\n\n# Set up the Sentinel client\nconfig = get_scraping_config(job_id, logger, sentinel_client_id, sentinel_client_secret)\n\n# Create a ScrapedDataRepository instance (implement as needed)\nscraped_data_repository = ScrapedDataRepository()\n\n# Run the scraper\noutput = scrape(job_id, tracer_id, scraped_data_repository, log_level, long_left, lat_down, long_right, lat_up, config, start_date, end_date, image_dir, resolution)\n\n# Output the result\nprint(output)\n'})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>c});var r=i(6540);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);